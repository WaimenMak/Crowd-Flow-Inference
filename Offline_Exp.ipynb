{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:17.549840Z",
     "start_time": "2024-05-23T09:47:16.605607Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import process_sensor_data\n",
    "df_dict = {}\n",
    "# Define path to parent directory containing subdirectories with CSV files\n",
    "parent_dir = 'sc_sensor'\n",
    "# adding to the df_dict\n",
    "# Loop through each subdirectory in the parent directory\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from lib.utils import gen_data_dict, seperate_up_down\n",
    "import pickle\n",
    "# dataset_name = \"train_station\"\n",
    "dataset_name = \"maze\"\n",
    "if dataset_name != \"maze\":\n",
    "    df_dict = process_sensor_data(parent_dir, df_dict)  # only process sensor 2\n",
    "    data_dict = gen_data_dict(df_dict)\n",
    "    # process data dict to get the upstream and downstream data\n",
    "    from lib.utils import seperate_up_down, generating_ood_dataset\n",
    "    data_dict = seperate_up_down(data_dict)\n",
    "elif dataset_name == \"maze\":\n",
    "    with open(\"./sc_sensor/maze/flow_data.pkl\", \"rb\") as f:\n",
    "        data_dict = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:18.160501Z",
     "start_time": "2024-05-23T09:47:18.156229Z"
    }
   },
   "id": "2c54817c37fb7bcd",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from lib.dataloader import FlowDataset\n",
    "from lib.utils import gen_data_dict, process_sensor_data, StandardScaler, sliding_win\n",
    "from lib.utils import generating_ood_dataset, seperate_up_down, generating_insample_dataset, get_trainable_params_size\n",
    "import dgl\n",
    "import torch\n",
    "from dgl.data.utils import load_graphs\n",
    "# import pickle\n",
    "# set seed\n",
    "torch.manual_seed(1)\n",
    "#normalization\n",
    "# x_scalar = StandardScaler(mean=np.concatenate([x_train, x_val]).mean(),\n",
    "#                           std=np.concatenate([x_train, x_val]).std())\n",
    "\n",
    "# dataset_name = \"crossroad\"\n",
    "\n",
    "g_data = load_graphs('./graphs/graphs.bin')    \n",
    "if dataset_name == \"crossroad\":\n",
    "    # file_path = 'graphs/graph_data_crossroad.pkl'\n",
    "    # with open(file_path, 'rb') as file:\n",
    "    #     g = pickle.load(file)\n",
    "    g = g_data[0][0]\n",
    "\n",
    "elif dataset_name == \"train_station\":\n",
    "    # file_path = 'graphs/graph_data_trainstation.pkl'\n",
    "    # with open(file_path, 'rb') as file:\n",
    "    #     g = pickle.load(file)\n",
    "    g = g_data[0][1]\n",
    "\n",
    "elif dataset_name == \"maze\":\n",
    "    # file_path = 'graphs/graph_data_maze.pkl'\n",
    "    # with open(file_path, 'rb') as file:\n",
    "    #     g = pickle.load(file)\n",
    "    g = g_data[0][2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:18.924757Z",
     "start_time": "2024-05-23T09:47:18.918627Z"
    }
   },
   "id": "227ca6c2e218e294",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if dataset_name == \"crossroad\":\n",
    "    train_sc = ['sc_sensor/crossroad2']\n",
    "    test_sc = ['sc_sensor/crossroad2_2']\n",
    "\n",
    "if dataset_name == \"train_station\":\n",
    "    train_sc = ['sc_sensor/train6']\n",
    "    test_sc = ['sc_sensor/train3']\n",
    "    \n",
    "if dataset_name == \"maze\":\n",
    "    train_sc = ['sc_sensor/maze0']\n",
    "    test_sc = ['sc_sensor/maze0', 'sc_sensor/maze5','sc_sensor/maze8', 'sc_sensor/maze9',\n",
    "               'sc_sensor/maze10', ]\n",
    "\n",
    "# x_train, y_train, x_val, y_val, x_test, y_test = generating_ood_dataset(data_dict, train_sc, test_sc, lags=5, horizons=pred_horizon, shuffle=True)\n",
    "# x_train, y_train, x_val, y_val, x_test, y_test = generating_insample_dataset(data_dict, train_sc,\n",
    "#                                                                              lags=5,\n",
    "#                                                                              horizons=pred_horizon,\n",
    "#                                                                              portion=0.6,\n",
    "#                                                                              shuffle=True)\n",
    "\n",
    "chunk_size = 30\n",
    "lags = 5\n",
    "# '''Has to >= 2'''\n",
    "pred_horizon = 7 # 3, 5\n",
    "x_test, y_test = sliding_win(data_dict[\"sc_sensor/maze16\"], lags=lags, horizons=pred_horizon)\n",
    "num_input_timesteps = x_test.shape[1] # number of input time steps\n",
    "num_nodes = x_test.shape[2] # number of ancestor nodes, minus the down stream node\n",
    "# x_scalar = StandardScaler(mean=np.concatenate([x_train, x_val]).mean(),\n",
    "#                           std=np.concatenate([x_train, x_val]).std())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:19.769586Z",
     "start_time": "2024-05-23T09:47:19.764870Z"
    }
   },
   "id": "7f18436660df3f65",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from Diffusion_Network import Diffusion_Model\n",
    "# from Diffusion_Network2 import Diffusion_Model\n",
    "# from Diffusion_Network3_asc import Diffusion_Model\n",
    "from Diffusion_Network4 import Diffusion_Model\n",
    "# from Diffusion_Network_density import Diffusion_Model_Density\n",
    "# from Diffusion_Network_UQ import Diffusion_Model_UQ\n",
    "src, dst = g.edges()\n",
    "# model = Diffusion_Model_UQ(num_edges=len(src), num_timesteps_input=num_input_timesteps, graph=g, horizons=pred_horizon, scalar=None)\n",
    "\n",
    "# model = Diffusion_Model_Density(num_edges=len(src), num_timesteps_input=num_input_timesteps, graph=g, horizons=pred_horizon, scalar=None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Diffusion_Model(num_edges=len(src), num_timesteps_input=x_test.shape[1], graph=g, horizons=pred_horizon, device=device, scalar=None)\n",
    "# chunk_size = 30\n",
    "# lags = 5\n",
    "# '''Has to >= 2'''\n",
    "# pred_horizon = 5 # 3, 5\n",
    "# model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_model_network3.pth\"))\n",
    "if dataset_name == \"crossroad\":\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_model_network3_cross.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/online_diffusion.pt\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/offline_diffusion_uq_cross.pth\"))\n",
    "    model.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "if dataset_name == \"train_station\":\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_model_network4.pth\"))\n",
    "    model.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/online_diffusion_density.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_model_density.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_uq.pth\"))\n",
    "if dataset_name == \"maze\":\n",
    "    model.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"./checkpoint/diffusion/diffusion_model_network4_maze.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:21.557228Z",
     "start_time": "2024-05-23T09:47:21.085183Z"
    }
   },
   "id": "24910b08714fada7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    x_up = torch.FloatTensor(x_test[..., src].transpose(2, 0, 1)) # [num of src, batch_size, num_timesteps_input], num of src = num of dst = num of edges\n",
    "    x_down = torch.FloatTensor(x_test[..., dst].transpose(2, 0, 1)) # [num of dst, batch_size, num_timesteps_input]\n",
    "    x = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "    y = torch.FloatTensor(y_test.transpose(2, 0, 1))\n",
    "    g.ndata['feature'] = x # [node, batch_size, num_timesteps_input]\n",
    "    g.ndata['label'] = y # [node, batch_size, pred_horizon]\n",
    "    # v = model.velocity_model(x_up, x_down).numpy()\n",
    "    model(g.ndata['feature'][src], g.ndata['feature'][dst])\n",
    "    # flow = torch.sum(x_up, dim=2).numpy()\n",
    "    total_flow = torch.mean(torch.cat([x_up, x_down], dim=-1), dim=-1).numpy()\n",
    "flow = model.g.edata['message']\n",
    "speed = model.g.edata['v']\n",
    "F = model.g.edata['F']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:48:58.344099Z",
     "start_time": "2024-05-23T09:48:58.121737Z"
    }
   },
   "id": "6d7ac015dcd97c79",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:35.649629Z",
     "start_time": "2024-05-23T09:47:35.508902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "src_point =22\n",
    "dst_point = 2\n",
    "edge_id = np.where((src == src_point) & (dst == dst_point))[0]\n",
    "# plt.plot(flow[edge_id, :250].squeeze(), speed[edge_id, :250].squeeze(), '*')\n",
    "# plt.plot(flow[edge_id, 250:].squeeze(), speed[edge_id, 250:].squeeze(), '>')\n",
    "plt.xlabel('Flow')\n",
    "plt.ylabel('Speed')\n",
    "plt.plot(flow[edge_id, :].squeeze(), speed[edge_id, :].squeeze(), '>')"
   ],
   "id": "45d12c24fabc816b",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#netowrk2\n",
    "# g.ndata['feature'] = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "# pred = model.inference(g.ndata['feature'][src], g.ndata['feature'][dst])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:20:10.404291Z",
     "start_time": "2024-01-29T14:20:09.996740Z"
    }
   },
   "id": "8a1567ef452b6e03",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#network\n",
    "# g.ndata['feature'] = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "# g.ndata['label'] = torch.FloatTensor(y_test.transpose(2, 0, 1))\n",
    "# z = model.transition_probability(g.ndata['feature'])\n",
    "# g.ndata['embedding'] = z   # for attention\n",
    "# g.apply_edges(model.transition_probability.edge_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T10:30:30.144081Z",
     "start_time": "2024-01-29T10:30:30.128779Z"
    }
   },
   "id": "160b3b040683b99c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# generate matrix with the element is e\n",
    "with torch.no_grad():\n",
    "    num_edges = len(src)\n",
    "    sample_size = x_test.shape[0]\n",
    "    atten_mat = torch.zeros((num_nodes, num_nodes, sample_size))\n",
    "    for i in range(num_edges):\n",
    "        atten_mat[src[i], dst[i], :] = g.edata['e'][i, :]\n",
    "    # atten_mat = atten_mat.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T20:16:27.294549Z",
     "start_time": "2024-05-16T20:16:27.289893Z"
    }
   },
   "id": "12a693474e60ef75",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# generate matrix with the element is e\n",
    "np.around(atten_mat[..., -1].numpy(), 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T14:24:06.087611Z",
     "start_time": "2024-05-15T14:24:06.083209Z"
    }
   },
   "id": "b694930c8dc113d7",
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    num_edges = len(src)\n",
    "    sample_size = x_test.shape[0]\n",
    "    alpha_mat = torch.zeros((num_nodes, num_nodes, sample_size))\n",
    "    for i in range(num_edges):\n",
    "        # alpha_mat[src[i], dst[i], :] = g.edata['alpha'][i, :]\n",
    "        alpha_mat[src[i], dst[i], :] = g.edata['F'][i, :]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T20:16:07.365561Z",
     "start_time": "2024-05-16T20:16:07.361157Z"
    }
   },
   "id": "a32dcdc106b2b403",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(alpha_mat[..., -1], cmap=\"YlGnBu\")\n",
    "sns.heatmap(torch.mean(alpha_mat, dim=-1), cmap=\"YlGnBu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T20:16:09.438652Z",
     "start_time": "2024-05-16T20:16:09.230111Z"
    }
   },
   "id": "a95231379b51a934",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(atten_mat[..., -1], cmap=\"YlGnBu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T20:16:32.225043Z",
     "start_time": "2024-05-16T20:16:32.024160Z"
    }
   },
   "id": "8cada1902cf640df",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T20:18:48.130232Z",
     "start_time": "2024-05-16T20:18:47.928758Z"
    }
   },
   "cell_type": "code",
   "source": "sns.heatmap(np.multiply(atten_mat[..., -1].numpy(), alpha_mat[..., -1].numpy()), cmap=\"YlGnBu\")",
   "id": "4dc9f2a82a7d1907",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# Create a simple DGLGraph\n",
    "# num_nodes = 5\n",
    "# edges_src = [0, 1, 1, 2, 2, 3, 3, 4]\n",
    "# edges_dst = [1, 2, 3, 3, 4, 4, 0, 1]\n",
    "# graph = dgl.graph((edges_src, edges_dst), num_nodes=num_nodes)\n",
    "# graph.edata['features'] = torch.randn(8, 10)\n",
    "# src_idx, dst_idx = g.edges()\n",
    "src_idx = np.unique(src)\n",
    "# Get edge IDs going out from a specific source node\n",
    "edge_ids_dict = dict()\n",
    "for src_node_id in src_idx:\n",
    "    outbound_edges = np.where(np.array(src) == src_node_id)[0]\n",
    "    edge_ids_dict[src_node_id] = outbound_edges\n",
    "    print(src_node_id)\n",
    "    edge_data = g.edata['e'][outbound_edges, :]\n",
    "    print(torch.sum(edge_data))\n",
    "    # assert torch.sum(edge_data) != 354\n",
    "    print(\"Edge Data:\", edge_data.shape)\n",
    "    print(\"outbound_edges:\", outbound_edges)\n",
    "\n",
    "# Extract edge features and destination nodes connected to the specified source node\n",
    "# edge_data = graph.edata['features'][outbound_edges]\n",
    "# src_nodes = graph.edges()[0][outbound_edges]\n",
    "# dst_nodes = graph.edges()[1][outbound_edges]\n",
    "\n",
    "# print(\"Edge Data:\", edge_data)\n",
    "# print(\"Source Nodes:\", src_nodes)\n",
    "# print(\"Destination Nodes:\", dst_nodes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T23:27:30.484335Z",
     "start_time": "2024-01-24T23:27:30.465231Z"
    }
   },
   "id": "35de3b3a9850ac",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from scipy.stats import entropy\n",
    "# import torch.nn.functional as F\n",
    "# from torch import nn\n",
    "# # calculate the js divergence for each row\n",
    "# p = atten_mat[:, :, 50:53].numpy()\n",
    "# q = atten_mat[:, :, 51:54].numpy()\n",
    "# # p = p / np.sum(p, axis=1, keepdims=True)\n",
    "# # q = q / np.sum(q, axis=1, keepdims=True)\n",
    "# \n",
    "# # Calculate the average distribution\n",
    "# m = 0.5 * (p + q)\n",
    "# \n",
    "# # Calculate the Jensen-Shannon Divergence\n",
    "# jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "# entropy(p, q)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:03:05.221211Z",
     "start_time": "2024-01-19T23:03:05.212345Z"
    }
   },
   "id": "f86f14abdcd481d4",
   "execution_count": 96,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# p = atten_mat\n",
    "# q = atten_mat\n",
    "# m = 0.5 * (p + q)\n",
    "# def kl_divergence(p, q):\n",
    "#     return np.sum(np.where(p != 0, p * np.log(p / q), 0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:03:05.736376Z",
     "start_time": "2024-01-19T23:03:05.732924Z"
    }
   },
   "id": "859965cf71447c77",
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "edge_id = 3\n",
    "v = model.g.edata['v']\n",
    "plt.plot(flow[edge_id, :], v[edge_id, :], '*')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:52.096255Z",
     "start_time": "2024-05-23T09:47:52.005774Z"
    }
   },
   "id": "46e24925a0834bd6",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T09:47:56.802184Z",
     "start_time": "2024-05-23T09:47:56.537272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(flow.shape[0]):\n",
    "    plt.plot(flow[i, :], speed[i, :], 'o')"
   ],
   "id": "5a268b5fd4190353",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(flow[0, :], speed[0, :], '*')\n",
    "plt.plot(flow[1, :], speed[1, :], 'o')\n",
    "plt.plot(flow[6, :], speed[6, :], 'o')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:24:22.753440Z",
     "start_time": "2024-05-20T14:24:22.655378Z"
    }
   },
   "id": "3d6077071010fd7e",
   "execution_count": 200,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(total_flow.shape[0]):\n",
    "    plt.plot(total_flow[i, :], v[i, :], 'o')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T09:49:07.517446Z",
     "start_time": "2024-05-23T09:49:07.324086Z"
    }
   },
   "id": "4c0645680792421d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distribution plot of the velocity\n",
    "plt.hist(speed[1, :], bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:22:41.746301Z",
     "start_time": "2024-05-20T14:22:41.668875Z"
    }
   },
   "id": "6d4404c849a92c1c",
   "execution_count": 195,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.hist(speed[0, :], bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T20:20:07.094950Z",
     "start_time": "2024-05-16T20:20:07.021607Z"
    }
   },
   "id": "d861e41782d0dc82",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract x_up and x_down from the output\n",
    "with torch.no_grad():\n",
    "    v_model = model.velocity_model\n",
    "    up = v_model.linear12(v_model.relu(v_model.ln11(v_model.dropout(v_model.linear11(x_up)))))\n",
    "    up = torch.sigmoid(up)\n",
    "    \n",
    "    down = v_model.linear22(v_model.relu(v_model.ln21(v_model.dropout(v_model.linear21(x_down)))))\n",
    "    down = torch.sigmoid(down)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:22:52.357207Z",
     "start_time": "2024-05-20T14:22:52.344810Z"
    }
   },
   "id": "8a6ead55e1a40385",
   "execution_count": 197,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_up_flow = torch.sum(x_up, dim=2).numpy()\n",
    "up_flow_v = torch.sum(up, dim=2).numpy()\n",
    "total_down_flow = torch.sum(x_down, dim=2).numpy()\n",
    "down_flow_v = torch.sum(down, dim=2).numpy()\n",
    "for i in range(total_up_flow.shape[0]):\n",
    "    plt.plot(total_up_flow[i, :], up_flow_v[i, :], '*')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:22:53.024227Z",
     "start_time": "2024-05-20T14:22:52.876679Z"
    }
   },
   "id": "990911bfc2f3d0fe",
   "execution_count": 198,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(total_up_flow.shape[0]):\n",
    "    plt.plot(total_down_flow[i, :], down_flow_v[i, :], '*')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:22:56.148174Z",
     "start_time": "2024-05-20T14:22:56.015769Z"
    }
   },
   "id": "ae52b69154811a1a",
   "execution_count": 199,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "linear3_params = list(v_model.linear3.parameters())\n",
    "# Print the parameters or do something with them\n",
    "for param in linear3_params:\n",
    "    print(param)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:50:54.317077Z",
     "start_time": "2024-05-15T12:50:54.314021Z"
    }
   },
   "id": "eaa180383c1dfec7",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f18a13300965c0b"
  },
  {
   "cell_type": "code",
   "source": [
    "from lib.metric import masked_rmse_np, masked_mae_np, rho_risk, weighted_average_loss\n",
    "x_test, y_test = sliding_win(data_dict['sc_sensor/train10'], lags=lags, horizons=pred_horizon)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T12:56:01.825333Z",
     "start_time": "2024-05-20T12:56:01.820760Z"
    }
   },
   "id": "a956ae767efc35e8",
   "execution_count": 170,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with torch.no_grad():\n",
    "    # x_up = torch.FloatTensor(x_test[..., src].transpose(2, 0, 1)) # [num of src, batch_size, num_timesteps_input], num of src = num of dst = num of edges\n",
    "    # x_down = torch.FloatTensor(x_test[..., dst].transpose(2, 0, 1)) # [num of dst, batch_size, num_timesteps_input]\n",
    "    x = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "    y = torch.FloatTensor(y_test.transpose(2, 0, 1))\n",
    "    g.ndata['feature'] = x # [node, batch_size, num_timesteps_input]\n",
    "    g.ndata['label'] = y # [node, batch_size, pred_horizon]\n",
    "    # v = model.velocity_model(x_up, x_down).numpy()\n",
    "    pred, multi_step_pred = model.inference(g.ndata['feature'][src], g.ndata['feature'][dst])\n",
    "step = -1\n",
    "node_id = 22\n",
    "\n",
    "# label = g.ndata['label'][:,:, 0].numpy()\n",
    "multi_step_pred = multi_step_pred.numpy()\n",
    "label = y_test.transpose(2, 0, 1)\n",
    "# label2 = y_test2.transpose(2, 0, 1)\n",
    "x = np.arange(0, label.shape[1])\n",
    "\n",
    "#adjust figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.ylim(-5, 30)\n",
    "plt.plot(x, multi_step_pred[node_id, :, step])\n",
    "plt.plot(x, label[node_id, :, step])\n",
    "rmse = masked_rmse_np(multi_step_pred[node_id, :, step], label[node_id, :, step])\n",
    "rt = np.sum(rho_risk(multi_step_pred[node_id, :], label[node_id, :], timespan=3, rho=0.9))\n",
    "# wt = weighted_average_loss(pred, label, rho=0.9, timespan=2)\n",
    "plt.legend(['Prediction', 'Ground Truth'])\n",
    "plt.title(f'RMSE: {rmse}')\n",
    "print(f\"Rho Risk: {rt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:18:11.629432Z",
     "start_time": "2024-05-20T14:18:11.184064Z"
    }
   },
   "id": "e2d59c3faca33ca3",
   "execution_count": 178,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# UQ\n",
    "from Diffusion_Network4_UQ import Diffusion_Model_UQ\n",
    "diffusion_model_uq = Diffusion_Model_UQ(num_edges=len(src), num_timesteps_input=x_test.shape[1], graph=g, horizons=pred_horizon, scalar=None, device=device)\n",
    "\n",
    "diffusion_model_uq.load_state_dict(torch.load('./checkpoint/diffusion/offline_diffusion_uq4_train.pth'))\n",
    "if dataset_name == \"crossroad\":\n",
    "    diffusion_model_uq.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_UQ_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "if dataset_name == \"train_station\":\n",
    "    diffusion_model_uq.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_UQ_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "if dataset_name == \"maze\":\n",
    "    diffusion_model_uq.load_state_dict(torch.load(f\"./checkpoint/diffusion/Online_Diffusion_UQ_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    pred, multi_step_pred = diffusion_model_uq.inference(g.ndata['feature'][src], g.ndata['feature'][dst])\n",
    "    pred, multi_step_pred = pred.numpy(), multi_step_pred.numpy()\n",
    "label = y_test.transpose(2, 0, 1)\n",
    "# label2 = y_test2.transpose(2, 0, 1)\n",
    "x = np.arange(0, label.shape[1])\n",
    "\n",
    "#adjust figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "# plt.plot(x, pred[id, :, 0])\n",
    "# plt.plot(x, label[id, :, 0])\n",
    "plt.plot(x, multi_step_pred[node_id, :, step])\n",
    "plt.plot(x, label[node_id, :, step])\n",
    "sigma = g.ndata['sigma'].numpy()\n",
    "sigma = np.sqrt(pred + pred ** 2 * sigma)  # not exactly the right way to visualize the uncertainty\n",
    "# plot prediction interval\n",
    "# std_dev = np.sqrt(pred[node_id, :] + pred[node_id, :]**2 * sigma[node_id, :])\n",
    "# plt.fill_between(x, pred[node_id, :] - 2 * sigma[node_id, :], pred[node_id, :] + 2 * std_dev, alpha=0.5)\n",
    "plt.fill_between(x, pred[node_id, :] - 2 * sigma[node_id, :], pred[node_id, :] + 2 * sigma[node_id, :], alpha=0.5)\n",
    "plt.ylim(-5, 40)\n",
    "# plt.plot(x, v)\n",
    "rmse = masked_rmse_np(multi_step_pred[node_id, :, step], label[node_id, :, step])\n",
    "rt = np.sum(rho_risk(multi_step_pred[node_id, :], label[node_id, :], timespan=2, rho=0.9))\n",
    "# wt = weighted_average_loss(pred.T, label.T, rho=0.9, timespan=2)\n",
    "plt.title(f'RMSE: {rmse}')\n",
    "print(f\"Rho Risk: {rt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:16:46.312259Z",
     "start_time": "2024-05-20T14:16:45.673136Z"
    }
   },
   "id": "c7accc8beec1e224",
   "execution_count": 174,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.stats import nbinom\n",
    "ts = 270\n",
    "# id = 2\n",
    "# Set mean and standard deviation\n",
    "mean = pred[node_id, ts]\n",
    "std_dev = np.sqrt(pred[node_id, ts] + pred[node_id, ts]**2 * sigma[node_id, ts])\n",
    "\n",
    "# Calculate shape parameter (k)\n",
    "# k = (mean ** 2) / (std_dev ** 2)\n",
    "\n",
    "# Generate x values for the plot\n",
    "# x_values = np.arange(0, 2 * mean)\n",
    "x_values = np.arange(0, 70)\n",
    "# Calculate the probability mass function (PMF) for each x value\n",
    "pmf_values = nbinom.pmf(x_values, n=sigma[node_id, ts], p=sigma[node_id, ts]/(sigma[node_id, ts] + mean))\n",
    "# Plot the negative binomial distribution\n",
    "plt.bar(x_values, pmf_values, label=f'Negative Binomial (mean={mean:.2f}, std_dev={std_dev:.2f})', alpha=0.7)\n",
    "plt.title('Negative Binomial Distribution')\n",
    "plt.xlabel('Number of People (k)')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:17:34.420551Z",
     "start_time": "2024-05-20T14:17:34.279862Z"
    }
   },
   "id": "a9f58c6aa3130642",
   "execution_count": 175,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# g.ndata['feature'] = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "# g.ndata['label'] = torch.FloatTensor(y_test.transpose(2, 0, 1))\n",
    "# z = model.transition_probability(g.ndata['feature'])\n",
    "# g.ndata['embedding'] = z   # for attention\n",
    "# g.apply_edges(model.transition_probability.edge_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:27:07.975501Z",
     "start_time": "2024-01-29T14:27:07.913859Z"
    }
   },
   "id": "49967600aba22658",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# with torch.no_grad():\n",
    "#     num_edges = len(src)\n",
    "#     sample_size = x_test.shape[0]\n",
    "#     atten_mat = torch.zeros((num_nodes, num_nodes, sample_size))\n",
    "#     for i in range(num_edges):\n",
    "#         atten_mat[src[i], dst[i], :] = g.edata['e'][i, :]\n",
    "#     atten_mat = atten_mat.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T14:48:21.305764Z",
     "start_time": "2024-01-25T14:48:21.297417Z"
    }
   },
   "id": "706130b0c300bb0e",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:17:44.248459Z",
     "start_time": "2024-05-20T14:17:44.242173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LSTM\n",
    "from baselines.LSTM import SimpleLSTM\n",
    "lstm = SimpleLSTM(input_size=num_nodes, hidden_size=64, output_size=pred_horizon-1, num_layers=2, num_nodes=num_nodes)\n",
    "lstm.load_state_dict(torch.load(f\"./checkpoint/lstm/Online_LSTM_{dataset_name}_chunk{chunk_size}_lags{lags}_hor{pred_horizon}.pth\"))"
   ],
   "id": "2fdd5f5d28edf777",
   "execution_count": 176,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:17:45.212797Z",
     "start_time": "2024-05-20T14:17:45.046132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    multi_step_pred = lstm(torch.FloatTensor(x_test))\n",
    "    multi_step_pred = multi_step_pred.reshape(multi_step_pred.shape[0], pred_horizon-1, num_nodes).permute(2, 0, 1)\n",
    "    multi_step_pred = multi_step_pred.numpy()\n",
    "    label = y_test.transpose(2, 0, 1)\n",
    "    \n",
    "    x = np.arange(0, label.shape[1])\n",
    "    \n",
    "    #adjust figure size\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.ylim(-5, 40)\n",
    "    plt.plot(x, multi_step_pred[node_id, :, step])\n",
    "    plt.plot(x, label[node_id, :, step])\n",
    "    rmse = masked_rmse_np(multi_step_pred[node_id, :, step], label[node_id, :, step])\n",
    "    rt = np.sum(rho_risk(multi_step_pred[node_id, :], label[node_id, :], timespan=3, rho=0.9))\n",
    "    plt.legend(['Prediction', 'Ground Truth'])\n",
    "    plt.title(f'RMSE: {rmse}')\n",
    "print(f\"Rho Risk: {rt}\")"
   ],
   "id": "3dadeb053eb17f4b",
   "execution_count": 177,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# xgboost\n",
    "import xgboost as xgb\n",
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model('./checkpoint/xgboost/offline_xgboost_train_station.model')\n",
    "# loaded_model.load_model('./checkpoint/xgboost/xgboost_cross.model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:27:21.145065Z",
     "start_time": "2024-01-29T14:27:20.987237Z"
    }
   },
   "id": "be1b3a938e2b067",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "horizon = pred_horizon - 1\n",
    "x = x_test.reshape([-1, num_input_timesteps * num_nodes])\n",
    "# y = y_test.reshape([-1, pred_horizon * num_nodes])\n",
    "# y_test = y_test.reshape([-1, num_input_timesteps * num_nodes])\n",
    "pred = loaded_model.predict(xgb.DMatrix(x))\n",
    "# pred = pred.reshape([num_nodes, -1, horizon])\n",
    "\n",
    "pred = pred.reshape([-1, horizon, num_nodes]).transpose([2, 0, 1])\n",
    "# pred = pred.T\n",
    "x2 = np.arange(0, label.shape[1])\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x2, pred[id, :, step])  # node ts horizon\n",
    "plt.plot(x2, label[id, :, step])\n",
    "# plt.plot(x2, label[0, :])\n",
    "rmse = masked_rmse_np(pred[id, :, step], label[id, :, step])\n",
    "# rt = np.sum(rho_risk(pred[id, :], label[id, :], timespan=2, rho=0.9))\n",
    "plt.title(f'RMSE: {rmse}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:27:21.771456Z",
     "start_time": "2024-01-29T14:27:21.676519Z"
    }
   },
   "id": "bf5fc6a8e0c59256",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# MA\n",
    "from baselines.MA import Moving_Average\n",
    "ma = Moving_Average(horizons=pred_horizon-1)\n",
    "multi_step_pred = ma.inference(g.ndata['feature']).numpy()\n",
    "label = y_test.transpose(2, 0, 1)\n",
    "# x2 = np.arange(0, label.shape[1])\n",
    "id = node_id\n",
    "# step = -1\n",
    "x = np.arange(0, label.shape[1])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.ylim(-5, 40)\n",
    "plt.plot(x, multi_step_pred[id, :, step])  # node ts horizon\n",
    "plt.plot(x, label[id, :, step])\n",
    "# plt.plot(x2, label[0, :])\n",
    "rmse = masked_rmse_np(multi_step_pred[node_id, :, step], label[node_id, :, step])\n",
    "rt = np.sum(rho_risk(multi_step_pred[node_id, :], label[node_id, :], timespan=3, rho=0.9))\n",
    "plt.legend(['Prediction', 'Ground Truth'])\n",
    "plt.title(f'RMSE: {rmse}')\n",
    "print(f\"Rho Risk: {rt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:46:46.739957Z",
     "start_time": "2024-05-19T21:46:46.599761Z"
    }
   },
   "id": "823001ffeb45b2fc",
   "execution_count": 135,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# gat\n",
    "from baselines.GAT import GAT\n",
    "adj_mat = g.adjacency_matrix(transpose=False, scipy_fmt=\"coo\")\n",
    "gat_model = GAT(g=adj_mat, seq_len=num_input_timesteps, feature_size=1, hidden_dim=32, out_dim=pred_horizon-1, nodes=num_nodes, num_heads=3)\n",
    "gat_model.load_state_dict(torch.load(\"./checkpoint/gat/gat_trainstation.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:23:07.179336Z",
     "start_time": "2024-01-29T15:23:07.155293Z"
    }
   },
   "id": "ab3b41caa18b9184",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "id = 2\n",
    "step = 3\n",
    "# up = torch.FloatTensor(x_test.transpose(2, 0, 1))[src]\n",
    "# down = torch.FloatTensor(x_test.transpose(2, 0, 1))[dst]\n",
    "with torch.no_grad():\n",
    "    multi_step_pred = gat_model.inference(g.ndata['feature'].permute(1, 0, 2))\n",
    "    multi_step_pred = multi_step_pred.numpy()\n",
    "# plot prediction and ground truth\n",
    "\n",
    "# label = g.ndata['label'][:,:, 0].numpy()\n",
    "label = y_test.transpose(2, 0, 1)\n",
    "# label2 = y_test2.transpose(2, 0, 1)\n",
    "x = np.arange(0, label.shape[1])\n",
    "\n",
    "#adjust figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x, multi_step_pred[id, :, step])\n",
    "plt.plot(x, label[id, :, step])\n",
    "\n",
    "rmse = masked_rmse_np(multi_step_pred[id, :, step], label[id, :, step])\n",
    "# rt = np.sum(rho_risk(pred[id, :], label[id, :], timespan=2, rho=0.9))\n",
    "# wt = weighted_average_loss(pred.T, label.T, rho=0.9, timespan=2)\n",
    "plt.title(f'RMSE: {rmse}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T15:24:21.441866Z",
     "start_time": "2024-01-29T15:24:21.295741Z"
    }
   },
   "id": "c149bba335d13e20",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# gcn\n",
    "from baselines.GCN import GCN\n",
    "gcn_model = GCN(in_size=num_input_timesteps, hid_size=128, out_size=pred_horizon-1, scalar=None)  # out_size: prediction horizon\n",
    "gcn_model.load_state_dict(torch.load(\"./checkpoint/gcn/gcn_trainstation.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:50.400723Z",
     "start_time": "2024-01-29T14:52:50.321532Z"
    }
   },
   "id": "487b60f6f04ea94",
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "id = 23\n",
    "step = 3\n",
    "# up = torch.FloatTensor(x_test.transpose(2, 0, 1))[src]\n",
    "# down = torch.FloatTensor(x_test.transpose(2, 0, 1))[dst]\n",
    "g = dgl.add_self_loop(g)\n",
    "with torch.no_grad():\n",
    "    multi_step_pred = gcn_model.inference(g, g.ndata['feature'])\n",
    "    multi_step_pred = multi_step_pred.numpy()\n",
    "# plot prediction and ground truth\n",
    "\n",
    "# label = g.ndata['label'][:,:, 0].numpy()\n",
    "label = y_test.transpose(2, 0, 1)\n",
    "# label2 = y_test2.transpose(2, 0, 1)\n",
    "x = np.arange(0, label.shape[1])\n",
    "\n",
    "#adjust figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x, multi_step_pred[id, :, step])\n",
    "plt.plot(x, label[id, :, step])\n",
    "\n",
    "rmse = masked_rmse_np(multi_step_pred[id, :, step], label[id, :, step])\n",
    "# rt = np.sum(rho_risk(pred[id, :], label[id, :], timespan=2, rho=0.9))\n",
    "# wt = weighted_average_loss(pred.T, label.T, rho=0.9, timespan=2)\n",
    "plt.title(f'RMSE: {rmse}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:51.656925Z",
     "start_time": "2024-01-29T14:52:51.541860Z"
    }
   },
   "id": "fe78b177b8c0a8eb",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize Speed and Flow",
   "id": "3cb792956eea3e41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T15:10:26.790691Z",
     "start_time": "2024-05-20T15:10:26.524517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_test, y_test = sliding_win(data_dict['sc_sensor/maze14'], lags=lags, horizons=pred_horizon)\n",
    "with torch.no_grad():\n",
    "    x_up = torch.FloatTensor(x_test[..., src].transpose(2, 0, 1)) # [num of src, batch_size, num_timesteps_input], num of src = num of dst = num of edges\n",
    "    x_down = torch.FloatTensor(x_test[..., dst].transpose(2, 0, 1)) # [num of dst, batch_size, num_timesteps_input]\n",
    "    x = torch.FloatTensor(x_test.transpose(2, 0, 1))\n",
    "    y = torch.FloatTensor(y_test.transpose(2, 0, 1))\n",
    "    g.ndata['feature'] = x # [node, batch_size, num_timesteps_input]\n",
    "    g.ndata['label'] = y # [node, batch_size, pred_horizon]\n",
    "    # v = model.velocity_model(x_up, x_down).numpy()\n",
    "    model(g.ndata['feature'][src], g.ndata['feature'][dst])\n",
    "    # flow = torch.sum(x_up, dim=2).numpy()\n",
    "    # total_flow = torch.sum(torch.cat([x_up, x_down], dim=-1), dim=-1).numpy()\n",
    "flow = model.g.edata['message']\n",
    "speed = model.g.edata['v']\n",
    "F = model.g.edata['F']"
   ],
   "id": "ab2daae8542713b4",
   "execution_count": 223,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# speed and F\n",
    "import matplotlib.pyplot as plt\n",
    "time = np.arange(speed.shape[1])\n",
    "src_idx = 10\n",
    "dst_idx = 6\n",
    "edge_id = np.where((src == src_idx) & (dst == dst_idx))[0]\n",
    "plt.plot(time, speed[edge_id, :].squeeze())\n",
    "plt.plot(time, F[edge_id, :].squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:11:18.676465Z",
     "start_time": "2024-05-20T15:11:18.583827Z"
    }
   },
   "id": "e37430ac7727998c",
   "execution_count": 226,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T15:11:22.376747Z",
     "start_time": "2024-05-20T15:11:22.282169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flow\n",
    "plt.plot(time, flow[edge_id, :].squeeze())"
   ],
   "id": "5defe4d6bce232f9",
   "execution_count": 227,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# T = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], [0.1, 0.2, 0.3, 0.4, 0.5]])\n",
    "# alpha = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "# F = 1/(1 + alpha * T)\n",
    "# # f = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "# n = torch.tensor([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "# for i in zip(n, F):\n",
    "#     print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T10:51:41.206729Z",
     "start_time": "2024-01-17T10:51:41.172672Z"
    }
   },
   "id": "1039b29b44372e4d",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# src = np.array([0, 0, 0, 3, 3, 3, 5, 5, 5, 6, 6, 6])\n",
    "# dst = np.array([4, 2, 7, 1, 4, 7, 2, 7, 1, 2, 4, 1])\n",
    "# g = dgl.graph((src, dst))\n",
    "# g.edata['distance'] = torch.FloatTensor([43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43]) # 50m\n",
    "# # save the graph\n",
    "# with open('graphs/graph_data.pkl', 'wb') as file:\n",
    "#     pickle.dump(g, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T10:52:05.579069Z",
     "start_time": "2024-01-17T10:52:05.543548Z"
    }
   },
   "id": "9a19f0c8b41d108b",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \n",
    "v = 40\n",
    "alpha = np.arange(-1, 1, 0.01)\n",
    "F = 1/(1+alpha*v)\n",
    "plt.plot(alpha, F)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T10:21:51.491670Z",
     "start_time": "2024-05-15T10:21:51.403722Z"
    }
   },
   "id": "8f61983479586bca",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "v = 3\n",
    "alpha = np.arange(0.1, 1, 0.01)\n",
    "F = 1/(1+alpha*v)\n",
    "plt.plot(alpha, F)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T16:01:19.120293Z",
     "start_time": "2024-01-18T16:01:19.057080Z"
    }
   },
   "id": "40f9a327ee699016",
   "execution_count": 65,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
