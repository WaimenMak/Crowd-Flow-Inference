{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:19.287278Z",
     "start_time": "2023-11-27T13:01:17.630750Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import process_sensor_data\n",
    "df_dict = {}\n",
    "# Define path to parent directory containing subdirectories with CSV files\n",
    "parent_dir = './sc sensor'\n",
    "# adding to the df_dict\n",
    "# Loop through each subdirectory in the parent directory\n",
    "df_dict = process_sensor_data(parent_dir, df_dict)  # only process sensor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from lib.utils import gen_data_dict\n",
    "data_dict = gen_data_dict(df_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:19.291189Z",
     "start_time": "2023-11-27T13:01:19.286735Z"
    }
   },
   "id": "2c54817c37fb7bcd"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0, 59, 70, 65, 74, 55, 67, 54, 79, 69, 68, 63, 66, 73, 67, 64,  0,\n        0,  0,  0,  0,  0,  0,  0,  0])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['./sc sensor/crossroad1'][:,0,0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:19.296110Z",
     "start_time": "2023-11-27T13:01:19.293340Z"
    }
   },
   "id": "ac38fb5ba8c514d2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_insample_dataset(upstream_data, downstream_data, save_mode=False):\n",
    "    \"\"\"\n",
    "    Upstream data: upstream sensor data (ts, node_num)\n",
    "    Downstream data: downstream sensor data (ts, 1)\n",
    "    \"\"\"\n",
    "    all_x, all_y = [], []\n",
    "\n",
    "    x_offsets = np.sort(\n",
    "            # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "            np.concatenate((np.arange(-7, 1, 1),))\n",
    "        )\n",
    "    # Predict the next 2 mins   \n",
    "    y_offsets = np.sort(np.arange(1, 9, 1))\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(upstream_data.shape[0]- abs(max(y_offsets)))\n",
    "\n",
    "    # max_t = abs(N - abs(max(y_offsets)))  # Exclusive\n",
    "    x, y = [], []\n",
    "    for t in range(min_t, max_t):\n",
    "        x_t = upstream_data[t + x_offsets, ...]\n",
    "        # also store the downstream data to last row of x_t\n",
    "        x_t = np.concatenate((x_t, downstream_data[t + x_offsets, ...]), axis=1)\n",
    "        y_t = downstream_data[t + y_offsets, ...]\n",
    "        x.append(x_t)\n",
    "        y.append(y_t)\n",
    "\n",
    "    x = np.stack(x, axis=0)\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    all_x.append(x)\n",
    "    all_y.append(y)\n",
    "\n",
    "    zipped_lists = list(zip(all_x, all_y))\n",
    "    random.shuffle(zipped_lists)  # shuffle data\n",
    "    all_x, all_y = zip(*zipped_lists)\n",
    "\n",
    "    x = np.concatenate(all_x, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "\n",
    "    # divide dataset\n",
    "    num_samples, num_nodes = x.shape[0], x.shape[1]  # num_samples = ts - 12*2 +1\n",
    "\n",
    "    len_train = round(num_samples * 0.7)\n",
    "    len_val = round(num_samples * 0.1)\n",
    "    # x_train: (num_samples, sliced_ts, num_nodes) y_train: (nums_samples, sliced_ts, 1)\n",
    "    x_train, y_train = x[: len_train, ...], y[: len_train, ...]\n",
    "    x_val, y_val = x[len_train: len_train + len_val, ...], y[len_train: len_train + len_val, ...]\n",
    "    x_test, y_test = x[len_train + len_val:, ...], y[len_train + len_val:, ...]\n",
    "            \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:26.013267Z",
     "start_time": "2023-11-27T13:01:26.009354Z"
    }
   },
   "id": "227ca6c2e218e294"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "up = data_dict['./sc sensor/crossroad1'][:,0,0].reshape(-1,1) # shape (ts, num_nodes)\n",
    "down = data_dict['./sc sensor/crossroad1'][:,1,1].reshape(-1,1) # shape (ts, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:27.219117Z",
     "start_time": "2023-11-27T13:01:27.211627Z"
    }
   },
   "id": "96ba6ed0ae6fad60"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = generate_insample_dataset(up, down)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:01:27.983204Z",
     "start_time": "2023-11-27T13:01:27.975105Z"
    }
   },
   "id": "2d4b118836039881"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "((7, 8, 2), (7, 8, 1))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:27:10.780023Z",
     "start_time": "2023-11-26T15:27:10.776575Z"
    }
   },
   "id": "8ef9914d38b29e86"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 3, 4, 5, 6, 7])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 - np.arange(8, 0, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:02:49.920482Z",
     "start_time": "2023-11-27T13:02:49.917645Z"
    }
   },
   "id": "f0e0c6a60f23df"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [4, 5, 0, 0],\n",
      "        [6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Suppose you have a list of variable-length sequences as tensors\n",
    "seq1 = torch.tensor([1, 2, 3])\n",
    "seq2 = torch.tensor([4, 5])\n",
    "seq3 = torch.tensor([6, 7, 8, 9])\n",
    "\n",
    "# Create a list of variable-length sequences\n",
    "sequences = [seq1, seq2, seq3]\n",
    "\n",
    "# Pad the sequences with zeros\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "# Print the padded sequences\n",
    "print(\"Padded Sequences:\")\n",
    "print(padded_sequences)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:33:44.296913Z",
     "start_time": "2023-11-27T13:33:44.275761Z"
    }
   },
   "id": "7b88a89b2fe8e48"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Lengths: [3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in sequences]\n",
    "print(\"Original Lengths:\", lengths)\n",
    "\n",
    "# You can also pack the padded sequences to deal with variable-length sequences in RNNs\n",
    "packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:33:57.869138Z",
     "start_time": "2023-11-27T13:33:57.836883Z"
    }
   },
   "id": "781e0e9416bd8cd9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5     , 0.25    , 0.125   , 0.0625  , 0.03125 , 0.015625])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diffusion_sequence_vectorized(F, n):\n",
    "    indices = np.arange(n + 1)\n",
    "    result = F * np.power(1 - F, indices)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "initial_value = 0.5\n",
    "sequence_length = 5\n",
    "\n",
    "result_sequence = diffusion_sequence_vectorized(initial_value, sequence_length)\n",
    "result_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:46:53.141792Z",
     "start_time": "2023-11-27T13:46:53.137261Z"
    }
   },
   "id": "6ad69af10705553"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.dataloader import FlowDataset\n",
    "from lib.utils import gen_data_dict, process_sensor_data, generate_insample_dataset\n",
    "from Diffusion import Diffusion_Model\n",
    "\n",
    "df_dict = {}\n",
    "# Define path to parent directory containing subdirectories with CSV files\n",
    "parent_dir = './sc sensor'\n",
    "# adding to the df_dict\n",
    "# Loop through each subdirectory in the parent directory\n",
    "df_dict = process_sensor_data(parent_dir, df_dict)\n",
    "\n",
    "data_dict = gen_data_dict(df_dict)\n",
    "up = data_dict['./sc sensor/crossroad1'][:,0,0].reshape(-1,1) # shape (ts, num_nodes)\n",
    "down = data_dict['./sc sensor/crossroad1'][:,1,1].reshape(-1,1) # shape (ts, 1)\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = generate_insample_dataset(up, down) # x_train : [batch_size, num_timesteps_input, num_nodes]\n",
    "num_input_timesteps = x_train.shape[1] # number of input time steps\n",
    "num_nodes = x_train.shape[2] - 1 # number of ancestor nodes, minus the down stream node\n",
    "\n",
    "train_dataset = FlowDataset(x_train, y_train, batch_size=2)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)\n",
    "model = Diffusion_Model(num_nodes=1, num_timesteps_input=x_train.shape[1])\n",
    "model.velocity_model.requires_grad_(False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "for epoch in range(100):\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        # training loop x: [batch_size, num_timesteps_input, num_nodes]\n",
    "        x_up = x[:, :, :-1].reshape(-1, num_input_timesteps, num_nodes)\n",
    "        x_down = x[:, :, -1].reshape(-1, num_input_timesteps, 1).repeat(1, 1, num_nodes)\n",
    "        pred = model(x_up, x_down)\n",
    "        loss = loss_fn(pred, y[:, 0, :])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "# test\n",
    "test_dataset = FlowDataset(x_test, y_test, batch_size=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "print('*************')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:17:29.597089Z",
     "start_time": "2023-11-27T18:17:29.593921Z"
    }
   },
   "id": "a1e458561f0100de"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[79, 69],\n        [69, 83],\n        [68, 73],\n        [63, 57],\n        [66, 67],\n        [73, 63],\n        [67, 61],\n        [64, 71]],\n\n       [[69, 83],\n        [68, 73],\n        [63, 57],\n        [66, 67],\n        [73, 63],\n        [67, 61],\n        [64, 71],\n        [ 0, 65]]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(F.size(0)):\n",
    "    n = torch.max((total_time_steps - T_idx[i]), torch.FloatTensor([1]))\n",
    "    indices = torch.arange(n.item()-1, -1, -1)\n",
    "    F_sequence = F[i] * torch.pow(1 - F[i], indices)\n",
    "    sequences.append(F_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T13:20:19.913706Z",
     "start_time": "2023-11-29T13:20:19.908471Z"
    }
   },
   "id": "eae7ba2a37e50f5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def diffusion_sequence(F, n):\n",
    "    indices = torch.arange(n.item()-1, -1, -1)\n",
    "    result = F * torch.pow(1 - F, indices)\n",
    "    return result\n",
    "\n",
    "for i in range(F.size(0)): # F: [batch_size, 1]\n",
    "    F_sequence = diffusion_sequence(F[i], torch.max((total_time_steps - T_idx[i]), torch.FloatTensor([1])))\n",
    "    sequences.append(F_sequence)\n",
    "\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0) # [batch_size, num_timesteps_input]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88223cd626395ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m n \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create indices tensor based on the maximum value in 'n'\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of indices:\u001B[39m\u001B[38;5;124m\"\u001B[39m, indices\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mTypeError\u001B[0m: arange() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example vector 'n'\n",
    "n = torch.tensor([3, 5, 2])\n",
    "\n",
    "# Create indices tensor based on the maximum value in 'n'\n",
    "indices = torch.arange(n, 10)\n",
    "\n",
    "print(\"Shape of indices:\", indices.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T20:29:56.273064Z",
     "start_time": "2023-11-29T20:29:56.270899Z"
    }
   },
   "id": "7583fa4c6e9f9341"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b85c76712bde7d66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
