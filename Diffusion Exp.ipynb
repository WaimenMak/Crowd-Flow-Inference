{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T21:29:47.032290Z",
     "start_time": "2023-11-30T21:29:45.430930Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import process_sensor_data\n",
    "df_dict = {}\n",
    "# Define path to parent directory containing subdirectories with CSV files\n",
    "parent_dir = './sc sensor'\n",
    "# adding to the df_dict\n",
    "# Loop through each subdirectory in the parent directory\n",
    "df_dict = process_sensor_data(parent_dir, df_dict)  # only process sensor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from lib.utils import gen_data_dict\n",
    "data_dict = gen_data_dict(df_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T21:29:47.036603Z",
     "start_time": "2023-11-30T21:29:47.033193Z"
    }
   },
   "id": "2c54817c37fb7bcd"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(25, 8, 3)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['./sc sensor/crossroad4'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:13:26.796825Z",
     "start_time": "2023-12-01T12:13:26.792076Z"
    }
   },
   "id": "ac38fb5ba8c514d2"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_insample_dataset(data_dict, save_mode=False):\n",
    "    \"\"\"\n",
    "    Upstream data: upstream sensor data (ts, node_num)\n",
    "    Downstream data: downstream sensor data (ts, 1)\n",
    "    \"\"\"\n",
    "    all_x, all_y = [], []\n",
    "    for scenario in data_dict.keys():\n",
    "        # scenario = './sc sensor/crossroad1'\n",
    "        data = data_dict[scenario]\n",
    "        upstream_data = data[:,0,0].reshape(-1,1)\n",
    "        downstream_data = data[:,1,1].reshape(-1,1)\n",
    "        x_offsets = np.sort(\n",
    "                # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "                np.concatenate((np.arange(-7, 1, 1),))\n",
    "            )\n",
    "        # Predict the next 2 mins   \n",
    "        y_offsets = np.sort(np.arange(1, 2, 1))\n",
    "        min_t = abs(min(x_offsets))\n",
    "        max_t = abs(upstream_data.shape[0]- abs(max(y_offsets)))\n",
    "    \n",
    "        # max_t = abs(N - abs(max(y_offsets)))  # Exclusive\n",
    "        x, y = [], []\n",
    "        for t in range(min_t, max_t):\n",
    "            x_t = upstream_data[t + x_offsets, ...]\n",
    "            # also store the downstream data to last row of x_t\n",
    "            x_t = np.concatenate((x_t, downstream_data[t + x_offsets, ...]), axis=1)\n",
    "            y_t = downstream_data[t + y_offsets, ...]\n",
    "            x.append(x_t)\n",
    "            y.append(y_t)\n",
    "\n",
    "        x = np.stack(x, axis=0)\n",
    "        y = np.stack(y, axis=0)\n",
    "    \n",
    "        all_x.append(x)\n",
    "        all_y.append(y)\n",
    "\n",
    "    zipped_lists = list(zip(all_x, all_y))\n",
    "    random.shuffle(zipped_lists)  # shuffle data\n",
    "    all_x, all_y = zip(*zipped_lists)\n",
    "\n",
    "    x = np.concatenate(all_x, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "\n",
    "    # divide dataset\n",
    "    num_samples = x.shape[0]  # num_samples = ts - 12*2 +1\n",
    "\n",
    "    len_train = round(num_samples * 0.7)\n",
    "    len_val = round(num_samples * 0.1)\n",
    "    # x_train: (num_samples, sliced_ts, num_nodes) y_train: (nums_samples, sliced_ts, 1)\n",
    "    x_train, y_train = x[: len_train, ...], y[: len_train, ...]\n",
    "    x_val, y_val = x[len_train: len_train + len_val, ...], y[len_train: len_train + len_val, ...]\n",
    "    x_test, y_test = x[len_train + len_val:, ...], y[len_train + len_val:, ...]\n",
    "            \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "df_dict = process_sensor_data(parent_dir, df_dict)\n",
    "data_dict = gen_data_dict(df_dict)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = generate_insample_dataset(data_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:25:14.806464Z",
     "start_time": "2023-12-01T12:25:14.745147Z"
    }
   },
   "id": "227ca6c2e218e294"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import process_sensor_data, gen_data_dict\n",
    "# Define path to parent directory containing subdirectories with CSV files\n",
    "parent_dir = './sc sensor/'\n",
    "df_dict = {}\n",
    "# adding to the df_dict\n",
    "# Loop through each subdirectory in the parent directory\n",
    "df_dict = process_sensor_data(parent_dir, df_dict)  # only process sensor 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:17:06.641907Z",
     "start_time": "2023-12-01T12:17:06.583522Z"
    }
   },
   "id": "c2ac8e77b29694ca"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "up = data_dict['./sc sensor/crossroad1'][:,0,0].reshape(-1,1) # shape (ts, num_nodes)\n",
    "down = data_dict['./sc sensor/crossroad1'][:,1,1].reshape(-1,1) # shape (ts, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:08:24.576955Z",
     "start_time": "2023-12-01T12:08:24.571720Z"
    }
   },
   "id": "96ba6ed0ae6fad60"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# x_train, y_train, x_val, y_val, x_test, y_test = generate_insample_dataset(up, down)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:08:25.742090Z",
     "start_time": "2023-12-01T12:08:25.733453Z"
    }
   },
   "id": "2d4b118836039881"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "data_dict = gen_data_dict(df_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:17:11.383416Z",
     "start_time": "2023-12-01T12:17:11.376541Z"
    }
   },
   "id": "8ef9914d38b29e86"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "(52, 8, 2)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:25:31.741170Z",
     "start_time": "2023-12-01T12:25:31.736370Z"
    }
   },
   "id": "56e09f2a2eed7cd4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [4, 5, 0, 0],\n",
      "        [6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Suppose you have a list of variable-length sequences as tensors\n",
    "seq1 = torch.tensor([1, 2, 3])\n",
    "seq2 = torch.tensor([4, 5])\n",
    "seq3 = torch.tensor([6, 7, 8, 9])\n",
    "\n",
    "# Create a list of variable-length sequences\n",
    "sequences = [seq1, seq2, seq3]\n",
    "\n",
    "# Pad the sequences with zeros\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "# Print the padded sequences\n",
    "print(\"Padded Sequences:\")\n",
    "print(padded_sequences)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:33:44.296913Z",
     "start_time": "2023-11-27T13:33:44.275761Z"
    }
   },
   "id": "7b88a89b2fe8e48"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Lengths: [3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in sequences]\n",
    "print(\"Original Lengths:\", lengths)\n",
    "\n",
    "# You can also pack the padded sequences to deal with variable-length sequences in RNNs\n",
    "packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:33:57.869138Z",
     "start_time": "2023-11-27T13:33:57.836883Z"
    }
   },
   "id": "781e0e9416bd8cd9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5     , 0.25    , 0.125   , 0.0625  , 0.03125 , 0.015625])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diffusion_sequence_vectorized(F, n):\n",
    "    indices = np.arange(n + 1)\n",
    "    result = F * np.power(1 - F, indices)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "initial_value = 0.5\n",
    "sequence_length = 5\n",
    "\n",
    "result_sequence = diffusion_sequence_vectorized(initial_value, sequence_length)\n",
    "result_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:46:53.141792Z",
     "start_time": "2023-11-27T13:46:53.137261Z"
    }
   },
   "id": "6ad69af10705553"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[79, 69],\n        [69, 83],\n        [68, 73],\n        [63, 57],\n        [66, 67],\n        [73, 63],\n        [67, 61],\n        [64, 71]],\n\n       [[69, 83],\n        [68, 73],\n        [63, 57],\n        [66, 67],\n        [73, 63],\n        [67, 61],\n        [64, 71],\n        [ 0, 65]]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(F.size(0)):\n",
    "    n = torch.max((total_time_steps - T_idx[i]), torch.FloatTensor([1]))\n",
    "    indices = torch.arange(n.item()-1, -1, -1)\n",
    "    F_sequence = F[i] * torch.pow(1 - F[i], indices)\n",
    "    sequences.append(F_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T13:20:19.913706Z",
     "start_time": "2023-11-29T13:20:19.908471Z"
    }
   },
   "id": "eae7ba2a37e50f5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def diffusion_sequence(F, n):\n",
    "    indices = torch.arange(n.item()-1, -1, -1)\n",
    "    result = F * torch.pow(1 - F, indices)\n",
    "    return result\n",
    "\n",
    "for i in range(F.size(0)): # F: [batch_size, 1]\n",
    "    F_sequence = diffusion_sequence(F[i], torch.max((total_time_steps - T_idx[i]), torch.FloatTensor([1])))\n",
    "    sequences.append(F_sequence)\n",
    "\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0) # [batch_size, num_timesteps_input]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88223cd626395ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m n \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create indices tensor based on the maximum value in 'n'\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of indices:\u001B[39m\u001B[38;5;124m\"\u001B[39m, indices\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mTypeError\u001B[0m: arange() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example vector 'n'\n",
    "n = torch.tensor([3, 5, 2])\n",
    "\n",
    "# Create indices tensor based on the maximum value in 'n'\n",
    "indices = torch.arange(n, 10)\n",
    "\n",
    "print(\"Shape of indices:\", indices.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T20:29:56.273064Z",
     "start_time": "2023-11-29T20:29:56.270899Z"
    }
   },
   "id": "7583fa4c6e9f9341"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b85c76712bde7d66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
